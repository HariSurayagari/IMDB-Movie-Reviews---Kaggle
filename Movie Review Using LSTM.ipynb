{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 256)         5120000   \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, None, 256)         525312    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 5,843,077\n",
      "Trainable params: 5,843,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 45s 323us/step - loss: 1.0039 - acc: 0.5993 - val_loss: 0.8349 - val_acc: 0.6526\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 44s 313us/step - loss: 0.7717 - acc: 0.6798 - val_loss: 0.8018 - val_acc: 0.6681\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 43s 304us/step - loss: 0.6853 - acc: 0.7135 - val_loss: 0.8022 - val_acc: 0.6721\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 42s 299us/step - loss: 0.6268 - acc: 0.7346 - val_loss: 0.8485 - val_acc: 0.6722\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 42s 298us/step - loss: 0.5813 - acc: 0.7499 - val_loss: 0.8970 - val_acc: 0.6621\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 41s 295us/step - loss: 0.5478 - acc: 0.7632 - val_loss: 0.9466 - val_acc: 0.6619\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 41s 295us/step - loss: 0.5196 - acc: 0.7737 - val_loss: 0.9796 - val_acc: 0.6699\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 41s 295us/step - loss: 0.4962 - acc: 0.7826 - val_loss: 1.0572 - val_acc: 0.6621\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 42s 296us/step - loss: 0.4770 - acc: 0.7903 - val_loss: 1.0909 - val_acc: 0.6615\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 42s 296us/step - loss: 0.4575 - acc: 0.7972 - val_loss: 1.1348 - val_acc: 0.6524\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 42s 303us/step - loss: 0.4412 - acc: 0.8046 - val_loss: 1.1813 - val_acc: 0.6494\n",
      "Epoch 12/20\n",
      "140454/140454 [==============================] - 43s 306us/step - loss: 0.4276 - acc: 0.8113 - val_loss: 1.2001 - val_acc: 0.6473\n",
      "Epoch 13/20\n",
      "140454/140454 [==============================] - 42s 299us/step - loss: 0.4171 - acc: 0.8163 - val_loss: 1.2617 - val_acc: 0.6472\n",
      "Epoch 14/20\n",
      "140454/140454 [==============================] - 42s 299us/step - loss: 0.4045 - acc: 0.8220 - val_loss: 1.2704 - val_acc: 0.6480\n",
      "Epoch 15/20\n",
      "140454/140454 [==============================] - 42s 301us/step - loss: 0.3960 - acc: 0.8247 - val_loss: 1.3182 - val_acc: 0.6438\n",
      "Epoch 16/20\n",
      "140454/140454 [==============================] - 42s 299us/step - loss: 0.3861 - acc: 0.8304 - val_loss: 1.3613 - val_acc: 0.6436\n",
      "Epoch 17/20\n",
      "140454/140454 [==============================] - 42s 299us/step - loss: 0.3795 - acc: 0.8332 - val_loss: 1.3915 - val_acc: 0.6421\n",
      "Epoch 18/20\n",
      "140454/140454 [==============================] - 42s 296us/step - loss: 0.3727 - acc: 0.8358 - val_loss: 1.4289 - val_acc: 0.6328\n",
      "Epoch 19/20\n",
      "140454/140454 [==============================] - 42s 296us/step - loss: 0.3674 - acc: 0.8390 - val_loss: 1.4518 - val_acc: 0.6403\n",
      "Epoch 20/20\n",
      "140454/140454 [==============================] - 42s 302us/step - loss: 0.3610 - acc: 0.8420 - val_loss: 1.4561 - val_acc: 0.6405\n",
      "       PhraseId  Sentiment\n",
      "0        156061          2\n",
      "1        156062          2\n",
      "2        156063          2\n",
      "3        156064          2\n",
      "4        156065          2\n",
      "5        156066          2\n",
      "6        156067          3\n",
      "7        156068          2\n",
      "8        156069          4\n",
      "9        156070          2\n",
      "10       156071          2\n",
      "11       156072          2\n",
      "12       156073          2\n",
      "13       156074          2\n",
      "14       156075          2\n",
      "15       156076          2\n",
      "16       156077          3\n",
      "17       156078          2\n",
      "18       156079          2\n",
      "19       156080          2\n",
      "20       156081          2\n",
      "21       156082          2\n",
      "22       156083          2\n",
      "23       156084          2\n",
      "24       156085          2\n",
      "25       156086          2\n",
      "26       156087          2\n",
      "27       156088          2\n",
      "28       156089          3\n",
      "29       156090          2\n",
      "...         ...        ...\n",
      "66262    222323          3\n",
      "66263    222324          2\n",
      "66264    222325          3\n",
      "66265    222326          3\n",
      "66266    222327          2\n",
      "66267    222328          2\n",
      "66268    222329          4\n",
      "66269    222330          4\n",
      "66270    222331          2\n",
      "66271    222332          4\n",
      "66272    222333          2\n",
      "66273    222334          2\n",
      "66274    222335          3\n",
      "66275    222336          4\n",
      "66276    222337          2\n",
      "66277    222338          3\n",
      "66278    222339          2\n",
      "66279    222340          2\n",
      "66280    222341          1\n",
      "66281    222342          1\n",
      "66282    222343          1\n",
      "66283    222344          2\n",
      "66284    222345          2\n",
      "66285    222346          2\n",
      "66286    222347          2\n",
      "66287    222348          1\n",
      "66288    222349          1\n",
      "66289    222350          1\n",
      "66290    222351          1\n",
      "66291    222352          1\n",
      "\n",
      "[66292 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# control GPU usage using tensorflow\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True  # allocate dynamically\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "trainRaw =pd.read_csv(r\"C:\\Users\\hari0\\Dropbox\\Random codes\\Movie Reviews\\train.tsv\",sep='\\t')\n",
    "testRaw =pd.read_csv(r\"C:\\Users\\hari0\\Dropbox\\Random codes\\Movie Reviews\\test.tsv\",sep='\\t')\n",
    "FinalResults = pd.read_csv(r\"C:\\Users\\hari0\\Dropbox\\Random codes\\Movie Reviews\\sampleSubmission.csv\",sep=',')\n",
    "#print(FinalResults)\n",
    "#FinalResults.drop(['Sentiment'], axis=1)\n",
    "\n",
    "\n",
    "text = ' '.join(trainRaw.loc[trainRaw.SentenceId == 4, 'Phrase'].values)\n",
    "text = [i for i in ngrams(text.split(), 3)]\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
    "full_text = list(trainRaw['Phrase'].values) + list(testRaw['Phrase'].values)\n",
    "vectorizer.fit(full_text)\n",
    "Ytrain = to_categorical(trainRaw['Sentiment'].values)\n",
    "\n",
    "\n",
    "embed_size = 300\n",
    "max_features = 20000\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1000\n",
    "num_classes=Ytrain.shape[1]\n",
    "\n",
    "\n",
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)\n",
    "\n",
    "train_tokenized = tk.texts_to_sequences(trainRaw['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(testRaw['Phrase'])\n",
    "print (train_tokenized)\n",
    "\n",
    "max_len = 50\n",
    "Xtrain = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "Xtest = pad_sequences(test_tokenized, maxlen = max_len)\n",
    "\n",
    "# train, test split\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(Xtrain, Ytrain, test_size = 0.1)\n",
    "\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_features,256,mask_zero=True))\n",
    "model.add(LSTM(256,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\n",
    "model.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(Xtrain, Ytrain, validation_data=(Xval, Yval),epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "results=model.predict(Xtest)\n",
    "results = np.round(np.argmax(results, axis=1)).astype(int)\n",
    "FinalResults['Sentiment'] = results\n",
    "print (FinalResults)\n",
    "FinalResults.to_csv(\"Submission.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
